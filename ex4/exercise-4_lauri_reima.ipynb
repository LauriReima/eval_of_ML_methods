{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TKO_7092 Evaluation of Machine Learning Methods 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Student name: Lauri Reima\n",
    "\n",
    "Student number: 2109673\n",
    "\n",
    "Student email: loreim@utu.fi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Complete the tasks given to you in the letter below. In your submission, explain clearly, precisely, and comprehensively why the cross-validation described in the letter failed, how cross-validation should be performed in the given scenario and why  your cross-validation will give a reliable estimate of the generalisation performance. Then implement the correct cross-validation for the scenario and report its results.\n",
    "\n",
    "Remember to follow all the general exercise guidelines that are stated in Moodle. Full points (2p) will be given for a submission that demonstrates a deep understanding of cross-validation on pair-input data and implements the requested cross-validation correctly (incl. reporting the results). Partial points (1p) will be given if there are small error(s) but the overall approach is correct. No points will be given if there are significant error(s).\n",
    "\n",
    "The deadline of this exercise is **Wednesday 21 February 2024 at 11:59 PM**. Please contact Juho Heimonen (juaheim@utu.fi) if you have any questions about this exercise.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dear Data Scientist,\n",
    "\n",
    "I have a long-term research project regarding a specific set of proteins. Currently I am attempting to discover small organic compounds that can bind strongly to these proteins and thus act as drugs. I have a list of over 100.000 potential drug molecules, but their affinities still need to be verified in the lab. Obviously I do not have the resources to measure all the possible drug-target pairs, so I need to prioritise. I have decided to do this with the use of machine learning, but I have encountered a problem.\n",
    "\n",
    "Here is what I have done so far: First I trained a K-nearest neighbours regressor with the parameter value K=10 using all the 400 measurements I had made in the lab, which comprise of all the 77 target proteins of interest but only 59 different drug molecules. Then I performed a leave-one-out cross-validation with this same data to estimate the generalisation performance of the model. I used C-index and got a stellar score above 90%. Finally I used the model to predict the affinities of the remaining drug molecules. The problem is: when I selected the highest predicted affinities and tried to verify them in the lab, I found that many of them are much lower in reality. My model clearly does not work despite the high cross-validation score.\n",
    "\n",
    "Please explain why my estimation failed and how leave-one-out cross-validation should be performed to get a reliable estimate. Also, implement the correct leave-one-out cross-validation and report its results. I need to know whether I am wasting my lab resources by using my model.\n",
    "\n",
    "The data I used to create my model is available in the files `input.data`, `output.data` and `pairs.data` for you to use. The first file contains the features of the pairs, whereas the second contains their affinities. The third file contains the identifiers of the drug and target molecules of which the pairs are composed. The files are paired, i.e. the i<sup>*th*</sup> row in each file is about the same pair.\n",
    "\n",
    "Looking forward to hearing from you soon.\n",
    "\n",
    "Yours sincerely, \\\n",
    "Bio Scientist\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the questions about cross-validation on pair-input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why did the estimation described in the letter fail?\n",
    "# How should leave-one-out cross-validation be performed in the given scenario and why?\n",
    "# Remember to provide comprehensive and precise arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I tried this and didn't get near to 0,9 c-index value, but the mistake you are making, dear Bio Scientist\n",
    ", is that you are using all of the data. When using leave-one-out you are using one sample as test data, and all the rest data as training material. There are essentially 4 ways to do a study like this: \n",
    "\n",
    "A: like you did \n",
    "\n",
    "B: separate the data with same compounds as the test sample from the training data\n",
    "\n",
    "C: separate the data with same proteins as the test sample from the training data\n",
    "\n",
    "D: separate the data with same compounds or same proteins as the test sample from the training data\n",
    "\n",
    "Below I have done a function which you can use to calculate C-indexes with a parameter A, B, C or D. \n",
    "\n",
    "But which one to use in your case? \n",
    "\n",
    "You have a lot of potential drugs and you don't really want to train your data with the same drugs affinities towards other proteins. You really want a 'new' drug on every iteration so you use opiton B. This way we avoid overfitting and get a more generalized performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries you need.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the utility functions you need in your analysis.\n",
    "def cindex(y, yp):\n",
    "    n = 0\n",
    "    h_num = 0 \n",
    "    for i in range(0, len(y)):\n",
    "        t = y[i]\n",
    "        p = yp[i]\n",
    "        for j in range(i+1, len(y)):\n",
    "            nt = y[j]\n",
    "            np = yp[j]\n",
    "            if (t != nt): \n",
    "                n = n + 1\n",
    "                if (p < np and t < nt) or (p > np and t > nt): \n",
    "                    h_num += 1\n",
    "                elif (p == np):\n",
    "                    h_num += 0.5\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    return h_num/n\n",
    "    \n",
    "# a function to calculate the indices needed in training\n",
    "def training_indices(data, ind):\n",
    "    # drugs are the same\n",
    "    cond_1 = data[0] == data.values[ind][0]\n",
    "    # targets are the same \n",
    "    cond_2 = data[1] == data.values[ind][1]\n",
    "    # \n",
    "    same_drug = data[cond_1]\n",
    "    same_target = data[cond_2]\n",
    "    # the indeces of the SAME drugs, targets and both\n",
    "    drug = np.array(same_drug.index)\n",
    "    target = np.array(same_target.index)\n",
    "    both = np.unique(np.concatenate((drug, target)))\n",
    "    # training indeces which are indeces other than those up\n",
    "    train_ind_all = np.delete(np.arange(len(data)), ind)\n",
    "    train_ind_drug = np.delete(np.arange(len(data)), drug)\n",
    "    train_ind_target = np.delete(np.arange(len(data)), target)\n",
    "    train_ind_both = np.delete(np.arange(len(data)), both)\n",
    "    # return a dictionary so we can use our wanted type\n",
    "    return {'A': train_ind_all, 'B':train_ind_drug, 'C':train_ind_target, 'D': train_ind_both}\n",
    "\n",
    "# basically a leave-one-out crossvalidation function with a little twist. We can choose the observation type as well\n",
    "# observation_type is by default: 'A'. Other choises are 'B', 'C' and 'D'\n",
    "def type_choice(input_data: np.array, output_data: np.array, pairs: pd.DataFrame, observation_type='A'): \n",
    "    knn = KNeighborsRegressor(n_neighbors=10)\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    # loop trough input_data\n",
    "    for i in range(len(input_data)):\n",
    "        # use the selfmade function to choose the training indeces, this might be a little expensive \n",
    "        train_ind = training_indices(pairs, i)[observation_type] \n",
    "        # make the training data\n",
    "        X_train, y_train = input_data[train_ind], output_data[train_ind]        \n",
    "        # fit the model\n",
    "        knn.fit(X_train, y_train)\n",
    "        # prediction made with the test material\n",
    "        y_pred = knn.predict(input_data[i].reshape(1, -1))\n",
    "        # add the predictions and true values to separate arrays\n",
    "        predictions.extend(y_pred) \n",
    "        true_values.extend(output_data[i]) \n",
    "    # and calculate the c-index value from the arrays    \n",
    "    c_index = cindex(true_values,predictions)\n",
    "\n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data files (input.data, output.data, pairs.data).\n",
    "input = pd.read_csv('input.data', sep=' ', header=None)\n",
    "output = pd.read_csv('output.data', header=None)\n",
    "pairs = pd.read_csv('pairs.data', sep=' ', header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement and run cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type B c-index: 0.51968671679198\n"
     ]
    }
   ],
   "source": [
    "# Implement and run the requested cross-validation. Report and interpret its results.\n",
    "input_data = input.values # numpy\n",
    "output_data = output.values # numpy\n",
    "                            # pairs is a dataframe\n",
    "#print(f'Type A c-index: {type_choice(input_data, output_data, pairs)}')\n",
    "print(f'Type B c-index: {type_choice(input_data, output_data, pairs, \"B\")}')\n",
    "#print(f'Type C c-index: {type_choice(input_data, output_data, pairs, \"C\")}')\n",
    "#print(f'Type D c-index: {type_choice(input_data, output_data, pairs, \"D\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C-index value of 0,52 indicates the algorithm is not very good at making predictions. \n",
    "It's just barely better than guessing. The model might be better with more data, but still it generalizes the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
